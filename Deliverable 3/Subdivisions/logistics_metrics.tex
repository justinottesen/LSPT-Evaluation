\subsection*{Team Members}

\textbf{Member:} Justin Ottesen \\
\smallskip\textbf{Role:} Team Lead \& Developer \\
\smallskip\textbf{Contributions:}
\\- Set up of repository, deployment of code on VM
\\- Set up GitHub actions, rules, project, and issues
\\- Wrote all code for project
\\- Wrote all unit and system tests
\\- Wrote Unit Tests section of deliverable 3
\\- Proofread rest of deliverable

\smallskip\textbf{Member:} Kenji Chevray \\
\smallskip\textbf{Role:} Developer \\
\smallskip\textbf{Contributions:}
\\- Wrote project postmortem
\\- Wrote summary of theoretical future steps
\\- Updated system and component tests


\smallskip\textbf{Member:} Nahuel Fernandez \\
\smallskip\textbf{Role:} Developer \\
\smallskip\textbf{Contributions:}
\\- Wrote the Team Collaboration Subsection
\\- Picked 6 new system tests to put in the pdf
\\- Wrote the System Integration Testing Section
\\- Did some formatting on the pdf 
\\- Updated system and component tests

\smallskip\textbf{Member:} Jayak Patel \\
\smallskip\textbf{Role:} Developer \\
\smallskip\textbf{Contributions:}
\\- Wrote the results of the Success Metrics
\\- Described the stubs that were implemented
\\- Updated system and component tests

\subsection*{Team Collaboration Information}
\href{https://discord.gg/pMyXUx2X}{Class Discord}: (External Communication)
\\Private Discord: (Internal Communication)
\\Team Drive Folder: (Intenal Prep Work)
\\GitHub Repository: \url{https://github.com/justinottesen/LSPT-Evaluation} 
\\Repo Includes detailed README files describing component, plan, and installation.

\pagebreak
\subsection*{Success Metrics}

\subsubsection*{Internal Success Metrics:}

\subsubsection*{Scalability:}
This success metric measured scalability, comparing the time it takes for a query 
to run based on the component's storage and the number of users using the search 
engine. Our component is very scalable in terms of storage, as it is a simple 
implementation and doesn't store anything. However, as it runs in one thread, the
number of concurrent users is limited. So, we have mixed results in terms of scalability, and if we 
implemented multiple threads, then we would improve our scalability.

\subsubsection*{Testability:}
The evaluation component can measure testability by monitoring the total test 
coverage and the success rate of the tests. Overall, our current testability is 
low, as we still have multiple requirements that we aren't fulfilling 
(finding synonyms for autofill, accepting "right to 
be forgotten", etc.). The success metric will increase as components contribute more 
features to the search engine and create test cases for them.

\subsubsection*{Usability:}
The evaluation component can measure the overall usability of the SE based on the 
feedback we receive from UI/UX. Although there isn't any negative feedback or bug 
reports, there aren't enough people using the search engine and sending bugs to 
determine the usability. As the project continues, the search engine should 
develop more and the usability success metric should increase. 

\subsubsection*{Internal Outcomes:}
Overall, these internal outcomes are low, as the beta release is intended to be
simpler, and having limited features. More work on our component is required, 
as well as more implementation on the component once the component is tested
and works at higher scales.


\subsection*{External Success Metric:}

\subsubsection*{Predictability:}
The evaluation component can measure predictability by measuring how often the 
autofill function is called before a query ID is generated. Currently, the get 
autofill results are limited, and cannot be improved through user data unless 
more users use the search engine and the search engine receives data that can be 
used for predictability, so currently the predictability is low. The 
predictability will increase once we start having users utilize the search engine 
and put their search history in the search engine.

\subsubsection*{Performance:}
Since the evaluation component sends out the query ID and receives the query ID at 
the end of the query runtime, it can conduct performance tests on the overall 
performance of the query runtime. Currently, our component connects with UI/UX,  
but UI/UX doesn't receive any results back yet. So, the performance is not 
working, which shows a low performance. Once UI/UX can return results, then we can 
measure the performance more accurately.

\subsubsection*{Relevancy:}
The relevancy success metric refers to the position of the link clicked on by the 
user. If the user clicks on the first few links, the search results are 
considered relevant and would have a high score. However, if the user clicks on a 
later link, or even goes to the next page, the search results would be considered 
irrelevant,  and therefore, would not be successful. Currently, our search engine 
does not show results. However, our program should normally be able to provide 
relevant results as the ranking component updates its algorithm. 

\subsubsection*{External Outcomes:}
Currently, there are other components that are not functioning. This results in 
the search engine not running, which means predictability, performance, and 
relevancy of the search engine on the user side should all be considered low. 
